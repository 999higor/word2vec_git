{"cells":[{"cell_type":"markdown","metadata":{},"source":["**Bibliotecas**"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","import re\n","from nltk.corpus import stopwords\n","from gensim.models import Word2Vec\n","import multiprocessing\n","from gensim import matutils  # utility fnc for pickling, common scipy operations etc\n","import time\n","\n","import unicodedata\n","import string\n","import textdistance"]},{"cell_type":"markdown","metadata":{},"source":["**Importação e limpeza dos dados**"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["C:\\Users\\higor\\AppData\\Local\\Temp\\ipykernel_15932\\196608791.py:5: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  df = df.append(df2)\n"]}],"source":["cols = ['produto', 'ncm']\n","# cols2 = ['produto' 'ncm']\n","df = pd.read_csv('C:/Users/higor/OneDrive/Área de Trabalho/db_tcc/cotri-nome-ncm.csv', header = 0, names = cols, engine = 'python', encoding = 'latin1')\n","df2 = pd.read_csv('C:/Users/higor/OneDrive/Área de Trabalho/db_tcc/nfe-nome-ncm.csv', header = 0, names = cols, engine = 'python', encoding = 'latin1')\n","df = df.append(df2)"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["stop_words = ['','ampgtembalagemcx12', 'ampgtembalagemdz12',\n","'ampgtembalagemcx48']"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2022-05-20T20:25:17.311029Z","iopub.status.busy":"2022-05-20T20:25:17.310748Z","iopub.status.idle":"2022-05-20T20:39:32.363438Z","shell.execute_reply":"2022-05-20T20:39:32.362673Z","shell.execute_reply.started":"2022-05-20T20:25:17.311001Z"},"trusted":true},"outputs":[],"source":["# csv_path = \"C:/Users/higor/OneDrive/Área de Trabalho/db_tcc/train.csv\"\n","# df = pd.read_csv(csv_path)\n","\n","# def clean_data(text):\n","#     text = re.sub(r'[^ \\nA-Za-z0-9À-ÖØ-öø-ÿ/]+', '', text)\n","#     text = re.sub(r'[\\\\/×\\^\\]\\[÷]', '', text)\n","#     return text\n","\n","def clean_data(text):\n","    try:\n","        text = unicode(text, 'utf-8')\n","    except NameError:\n","        pass\n","\n","    text = unicodedata.normalize('NFD', text)\\\n","           .encode('ascii', 'ignore')\\\n","           .decode(\"utf-8\")\n","\n","    text = text.translate(str.maketrans(string.punctuation, ' '*len(string.punctuation))) \n","    # text = text.translate(str.maketrans('', '', re.sub('/','', string.punctuation)))\n","\n","    # text = re.sub('\\s\\s+', ' ', text.strip())\n","    return str(text)\n","\n","def change_lower(text):\n","    text = text.lower()\n","    return text\n","\n","stopwords_list = stopwords.words(\"portuguese\")\n","# stopwords_list.append(stop_words)\n","def remover(text):\n","    text_tokens = text.split(\" \")\n","    final_list = [word for word in text_tokens if not word in stopwords_list]\n","    text = ' '.join(final_list)\n","    return text\n","\n","def get_w2vdf(df, column_name):\n","    w2v_df = pd.DataFrame(df[column_name]).values.tolist()\n","    for i in range(len(w2v_df)):\n","        w2v_df[i] = re.sub('\\s\\s+', ' ', w2v_df[i][0].strip()).split(\" \")\n","        # w2v_df[i] =  re.sub('+ ', '', w2v_df[i][0].strip()).split(\" \")\n","        # w2v_df[i] = w2v_df[i][0].split(\" \")\n","    return w2v_df\n"]},{"cell_type":"markdown","metadata":{},"source":["**Treinamento do modelo**"]},{"cell_type":"code","execution_count":113,"metadata":{},"outputs":[],"source":["def train_w2v(w2v_df):\n","    cores = multiprocessing.cpu_count()\n","    print(\"-- Cores:\",cores)\n","    w2v_model = Word2Vec(min_count=2, #before was = 4\n","                         window=5,\n","                         vector_size=300, #vector_size or vector\n","                         alpha=0.03, \n","                         min_alpha=0.0007, \n","                         sg = 1,\n","                         workers=cores-1)\n","    \n","    print(\"-- Building vocab\\n\")\n","    w2v_model.build_vocab(w2v_df, progress_per=10000)\n","    print(\"-- Training the model\\n\")\n","    w2v_model.train(w2v_df, total_examples=w2v_model.corpus_count, epochs=1000, report_delay=1)\n","    return w2v_model"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["# transform the text to string type, lower case, clean data\n","# and remove stop words\n","df[\"produto\"] = df[\"produto\"].astype(str)\n","df[\"produto\"] = df[\"produto\"].apply(change_lower)\n","df[\"produto\"] = df[\"produto\"].apply(clean_data)\n","df[\"produto\"] = df[\"produto\"].apply(remover)\n","\n","# separate the words from the phrase\n","w2v_df = get_w2vdf(df, 'produto')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["w2v_df"]},{"cell_type":"markdown","metadata":{},"source":["**Treinamento**\n"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"data":{"text/plain":["'w2v_v2w7s300e750'"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["model_name = \"w2v_v2w7s300e750\"\n","model_name"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# train the model using the parameters above\n","# after training save the model for pre-load\n","w2v_model = train_w2v(w2v_df)\n","print(\"-- Saving the model\\n\")\n","\n","# w2v_model.save(\"word2vec.model\")\n","\n","w2v_model.save(model_name+\".model\")"]},{"cell_type":"markdown","metadata":{},"source":["**Carrega o modelo pre-treinado**"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["# load the pre-trained model\n","# w2v_model = Word2Vec.load(\"word2vec.model\")\n","# w2v_model = Word2Vec.load(\"word2vec_v2.model\")\n","w2v_model = Word2Vec.load(model_name+\".model\")"]},{"cell_type":"markdown","metadata":{},"source":["**Removing infrequent words**"]},{"cell_type":"code","execution_count":153,"metadata":{},"outputs":[],"source":["# remove the words OOV (out of vocabulary) from the given text list\n","def remove_infrequent_words(dataframe):\n","    counter = 0\n","    for i in range(len(dataframe)):\n","        # need to put >list< because i'm modifying a list while iterating over it\n","        for word in list(dataframe[i]):\n","            if not word in w2v_model.wv.key_to_index:\n","                counter += 1\n","                dataframe[i].remove(word)\n","\n","    print(\"removed words: \",counter)\n","    return dataframe"]},{"cell_type":"code","execution_count":154,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["removed words:  34319\n"]}],"source":["w2v_df = remove_infrequent_words(w2v_df)"]},{"cell_type":"markdown","metadata":{},"source":["**Calcula o Data Frequency (DF)**"]},{"cell_type":"code","execution_count":155,"metadata":{},"outputs":[],"source":["## tentativa 04\n","## calcula o DF (data frequency) do dataframe\n","data_freq = {}\n","\n","w2v_df_len = len(w2v_df)\n","\n","for i in range(w2v_df_len):\n","    tokens = w2v_df[i]\n","    for w in tokens:\n","        try:\n","            data_freq[w].add(i)\n","        except:\n","            data_freq[w] = {i}\n","            \n","for i in data_freq:\n","    data_freq[i] = len(data_freq[i])"]},{"cell_type":"code","execution_count":156,"metadata":{},"outputs":[],"source":["def doc_freq(word):\n","    c = 0\n","    try:\n","        c = data_freq[word]\n","    except:\n","        pass\n","    return c"]},{"cell_type":"markdown","metadata":{},"source":["**Calcula o IDF**"]},{"cell_type":"code","execution_count":157,"metadata":{},"outputs":[],"source":["idf = {}\n","\n","for i in range(w2v_df_len):\n","    tokens = w2v_df[i]\n","    for token in np.unique(tokens):\n","        token_df = doc_freq(token)\n","        calc_idf = np.log10((w2v_df_len + 1) / (token_df + 1))\n","\n","        idf[token] = calc_idf"]},{"cell_type":"markdown","metadata":{},"source":["**Funções de Similaridade**"]},{"cell_type":"code","execution_count":123,"metadata":{},"outputs":[],"source":["def similarity_cosine(vec1, vec2):\n","    cosine_similarity = np.dot(matutils.unitvec(vec1), matutils.unitvec(vec2))\n","    return cosine_similarity"]},{"cell_type":"code","execution_count":81,"metadata":{},"outputs":[],"source":["# creates a representation of the sentence from the\n","# average of the sentence's word vectors \n","def average_sentence2vec(w2v_df):\n","    flag = True\n","    vector = 0\n","    for word in w2v_df:\n","        if(flag):\n","            vector = w2v_model.wv[word]\n","            flag = False\n","        else:\n","            vector = np.add(vector, w2v_model.wv[word])\n","    vector = vector / (len(w2v_df) + 1)\n","    return vector        "]},{"cell_type":"code","execution_count":82,"metadata":{},"outputs":[],"source":["# creates a representation of the sentence from the\n","# sum of the sentence's word vectors \n","def sum_sentence2vec(w2v_df):\n","    flag = True\n","    vector = 0\n","    for word in w2v_df:\n","        if(flag):\n","            vector = w2v_model.wv[word]\n","            flag = False\n","        else:\n","            vector = np.add(vector, w2v_model.wv[word])\n","    return vector        "]},{"cell_type":"code","execution_count":83,"metadata":{},"outputs":[],"source":["# creates a representation of the sentence from the\n","# sum of the sentence's word vectors \n","def weighted_sentence2vec(w2v_df):\n","    flag = True\n","    vector = 0\n","    for word in w2v_df:\n","        if(flag):\n","            vector = w2v_model.wv[word] * idf[word]\n","            flag = False\n","        else:\n","            vector = np.add(vector, w2v_model.wv[word] * idf[word])\n","    vector = vector / (len(w2v_df) + 1) ##verificar\n","    return vector        "]},{"cell_type":"code","execution_count":84,"metadata":{},"outputs":[],"source":["# calc the similarity between two sentences using cos\n","def average_sentence_similarity(vector1, vector2):\n","    try:\n","        similarity = np.dot(matutils.unitvec(vector1), matutils.unitvec(vector2))\n","    except:\n","        similarity = 0\n","    return similarity"]},{"cell_type":"code","execution_count":85,"metadata":{},"outputs":[],"source":["# calc the similarity between two sentences using cos\n","def sum_sentence_similarity(vector1, vector2):\n","    try:\n","        similarity = np.dot(matutils.unitvec(vector1), matutils.unitvec(vector2))\n","    except:\n","        similarity = 0\n","    return similarity"]},{"cell_type":"code","execution_count":86,"metadata":{},"outputs":[],"source":["def weighted_sentence_similarity(vector1, vector2):\n","    try:\n","        similarity = np.dot(matutils.unitvec(vector1), matutils.unitvec(vector2))\n","    except:\n","        similarity = 0\n","    return similarity"]},{"cell_type":"markdown","metadata":{},"source":["**Importando dados: NFE e Base**"]},{"cell_type":"code","execution_count":87,"metadata":{},"outputs":[],"source":["cols = ['produto', 'ncm']\n","# cols2 = ['produto' 'ncm']\n","df_base = pd.read_csv('C:/Users/higor/OneDrive/Área de Trabalho/db_tcc/cotri-nome-ncm.csv', header = 0, \n","names = cols, engine = 'python', encoding = 'utf-8')\n","df_nfe = pd.read_csv('C:/Users/higor/OneDrive/Área de Trabalho/db_tcc/nfe-nome-ncm.csv', header = 0, \n","names = cols, engine = 'python', encoding = 'utf-8')"]},{"cell_type":"markdown","metadata":{},"source":["**Limpeza: NFE e Base**"]},{"cell_type":"code","execution_count":88,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["removed words:  33685\n","removed words:  573\n"]}],"source":["# transform the text to string type, lower case, clean data\n","# and remove stop words\n","\n","column_name = 'produto'\n","# dataframe base\n","df_base['ncm'] = df_base['ncm'].replace('UPDT1', 0)\n","df_base['ncm'] = df_base['ncm'].fillna(0)\n","df_base['ncm'] = df_base['ncm'].astype(int)\n","df_base['produto_base'] = df_base[column_name]\n","df_base[column_name] = df_base[column_name].astype(str)\n","df_base[column_name] = df_base[column_name].apply(change_lower)\n","df_base[column_name] = df_base[column_name].apply(clean_data)\n","df_base[column_name] = df_base[column_name].apply(remover)\n","\n","# dataframe nfe\n","df_nfe['ncm'] = df_nfe['ncm'].astype(int)\n","df_nfe['produto_nfe'] = df_nfe[column_name]\n","df_nfe[column_name] = df_nfe[column_name].astype(str)\n","df_nfe[column_name] = df_nfe[column_name].apply(change_lower)\n","df_nfe[column_name] = df_nfe[column_name].apply(clean_data)\n","df_nfe[column_name] = df_nfe[column_name].apply(remover)\n","\n","# separate the words from the phrase\n","w2v_df_base = get_w2vdf(df_base, column_name)\n","\n","w2v_df_nfe = get_w2vdf(df_nfe, column_name)\n","\n","## removing infrequent words in the dataframe base and nfe\n","\n","w2v_df_base = remove_infrequent_words(w2v_df_base)\n","\n","w2v_df_nfe = remove_infrequent_words(w2v_df_nfe)"]},{"cell_type":"markdown","metadata":{},"source":["**Appending the dataframes**"]},{"cell_type":"code","execution_count":89,"metadata":{},"outputs":[],"source":["# append the w2v dataframe as a column in the df\n","\n","df_base['w2v'] = w2v_df_base\n","\n","df_nfe['w2v'] = w2v_df_nfe"]},{"cell_type":"markdown","metadata":{},"source":["**Criando o Dataframe**"]},{"cell_type":"code","execution_count":90,"metadata":{},"outputs":[],"source":["columns_db_final = ['produto_base', 'produto_nfe', 'w2v_base', 'w2v_nfe', 'ncm', 'score']\n","db_final = pd.DataFrame(columns=columns_db_final)"]},{"cell_type":"markdown","metadata":{},"source":["**Most Similar - w2v average**"]},{"cell_type":"code","execution_count":25,"metadata":{},"outputs":[],"source":["file_name = \"a_\"+model_name"]},{"cell_type":"code","execution_count":111,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["product with 0 similars: produto            saco pap 15kg 32x55   mil\n","ncm                                 48043190\n","produto_nfe        SACO PAP 15Kg 32x55 / MIL\n","w2v            [saco, pap, 15kg, 32x55, mil]\n","Name: 1358, dtype: object\n","\n"]}],"source":["db_final = pd.DataFrame(columns=columns_db_final)\n","for i in range(len(df_nfe)):\n","    \n","    score_final = 0\n","    score = 0\n","    first_element = True\n","    idx = 0\n","\n","    ncm = df_nfe.loc[i, 'ncm']\n","    df_temp = df_base.loc[df_base['ncm'] == ncm]\n","\n","    if len(df_temp) > 0:\n","\n","        for j in range(len(df_temp)):\n","\n","            if first_element:\n","                score_final = average_sentence_similarity(\n","                    average_sentence2vec(df_nfe.loc[i, 'w2v']), \n","                    average_sentence2vec(df_temp.loc[df_temp.index[j], 'w2v']))\n","                idx = df_temp.index[j]\n","                first_element = False\n","            else:\n","                score = average_sentence_similarity(\n","                    average_sentence2vec(df_nfe.loc[i, 'w2v']), \n","                    average_sentence2vec(df_temp.loc[df_temp.index[j], 'w2v']))\n","\n","            if score > score_final:\n","                score_final = score\n","                idx = df_temp.index[j]\n","\n","        # saving the best values in the dataframe\n","        try:\n","            db_final.loc[len(db_final.index)] = [\n","                df_temp.loc[idx, 'produto_base'],\n","                df_nfe.loc[i, 'produto_nfe'],\n","                df_temp.loc[idx, 'w2v'],\n","                df_nfe.loc[i, 'w2v'],\n","                df_nfe.loc[i, 'ncm'],\n","                score_final\n","            ]\n","        except:\n","            print(f\"something goes wrong: {df_temp.loc[idx,]}\\n{df_nfe.loc[i,]}\")\n","    \n","    else:\n","        print(f'product with 0 similars: {df_nfe.loc[i,]}\\n')\n","\n","db_final.to_csv('results/'+file_name+'.csv', encoding='utf-8')"]},{"cell_type":"markdown","metadata":{},"source":["**Most Similar - w2v weighted**"]},{"cell_type":"code","execution_count":136,"metadata":{},"outputs":[],"source":["file_name = \"w_\"+model_name\n"]},{"cell_type":"code","execution_count":137,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["product with 0 similars: produto            saco pap 15kg 32x55   mil\n","ncm                                 48043190\n","produto_nfe        SACO PAP 15Kg 32x55 / MIL\n","w2v            [saco, pap, 15kg, 32x55, mil]\n","Name: 1358, dtype: object\n","\n"]}],"source":["db_final = pd.DataFrame(columns=columns_db_final)\n","for i in range(len(df_nfe)):\n","    \n","    score_final = 0\n","    score = 0\n","    first_element = True\n","    idx = 0\n","\n","    ncm = df_nfe.loc[i, 'ncm']\n","    df_temp = df_base.loc[df_base['ncm'] == ncm]\n","\n","    if len(df_temp) > 0:\n","\n","        for j in range(len(df_temp)):\n","\n","            if first_element:\n","                score_final = weighted_sentence_similarity(\n","                    weighted_sentence2vec(df_nfe.loc[i, 'w2v']), \n","                    weighted_sentence2vec(df_temp.loc[df_temp.index[j], 'w2v']))\n","                idx = df_temp.index[j]\n","                first_element = False\n","            else:\n","                score = weighted_sentence_similarity(\n","                    weighted_sentence2vec(df_nfe.loc[i, 'w2v']), \n","                    weighted_sentence2vec(df_temp.loc[df_temp.index[j], 'w2v']))\n","\n","            if score > score_final:\n","                score_final = score\n","                idx = df_temp.index[j]\n","\n","        # saving the best values in the dataframe\n","        try:\n","            db_final.loc[len(db_final.index)] = [\n","                df_temp.loc[idx, 'produto_base'],\n","                df_nfe.loc[i, 'produto_nfe'],\n","                df_temp.loc[idx, 'w2v'],\n","                df_nfe.loc[i, 'w2v'],\n","                df_nfe.loc[i, 'ncm'],\n","                score_final\n","            ]\n","        except:\n","            print(f\"something goes wrong: {df_temp.loc[idx,]}\\n{df_nfe.loc[i,]}\")\n","    \n","    else:\n","        print(f'product with 0 similars: {df_nfe.loc[i,]}\\n')\n","\n","db_final.to_csv('results/'+file_name+'.csv', encoding='utf-8')"]},{"cell_type":"markdown","metadata":{},"source":["**Most Similar - w2v Sum**"]},{"cell_type":"code","execution_count":49,"metadata":{},"outputs":[],"source":["file_name = \"s_\"+model_name"]},{"cell_type":"code","execution_count":50,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["product with 0 similars: produto            saco pap 15kg 32x55   mil\n","ncm                                 48043190\n","produto_nfe        SACO PAP 15Kg 32x55 / MIL\n","w2v            [saco, pap, 15kg, 32x55, mil]\n","Name: 1358, dtype: object\n","\n"]}],"source":["db_final = pd.DataFrame(columns=columns_db_final)\n","for i in range(len(df_nfe)):\n","    \n","    score_final = 0\n","    score = 0\n","    first_element = True\n","    idx = 0\n","\n","    ncm = df_nfe.loc[i, 'ncm']\n","    df_temp = df_base.loc[df_base['ncm'] == ncm]\n","\n","    if len(df_temp) > 0:\n","\n","        for j in range(len(df_temp)):\n","\n","            if first_element:\n","                score_final = sum_sentence_similarity(\n","                    sum_sentence2vec(df_nfe.loc[i, 'w2v']), \n","                    sum_sentence2vec(df_temp.loc[df_temp.index[j], 'w2v']))\n","                idx = df_temp.index[j]\n","                first_element = False\n","            else:\n","                score = sum_sentence_similarity(\n","                    sum_sentence2vec(df_nfe.loc[i, 'w2v']), \n","                    sum_sentence2vec(df_temp.loc[df_temp.index[j], 'w2v']))\n","\n","            if score > score_final:\n","                score_final = score\n","                idx = df_temp.index[j]\n","\n","        # saving the best values in the dataframe\n","        try:\n","            db_final.loc[len(db_final.index)] = [\n","                df_temp.loc[idx, 'produto_base'],\n","                df_nfe.loc[i, 'produto_nfe'],\n","                df_temp.loc[idx, 'w2v'],\n","                df_nfe.loc[i, 'w2v'],\n","                df_nfe.loc[i, 'ncm'],\n","                score_final\n","            ]\n","        except:\n","            print(f\"something goes wrong: {df_temp.loc[idx,]}\\n{df_nfe.loc[i,]}\")\n","    \n","    else:\n","        print(f'product with 0 similars: {df_nfe.loc[i,]}\\n')\n","\n","db_final.to_csv('results/'+file_name+'.csv', encoding='utf-8')"]},{"cell_type":"markdown","metadata":{},"source":["**Most Similar - w2v average + func**"]},{"cell_type":"code","execution_count":97,"metadata":{},"outputs":[{"data":{"text/plain":["'a_jarowink_w2v_v2w7s300e750'"]},"execution_count":97,"metadata":{},"output_type":"execute_result"}],"source":["file_name = \"a_jarowink_\"+model_name\n","file_name"]},{"cell_type":"code","execution_count":96,"metadata":{},"outputs":[{"data":{"text/plain":["0.8755555555555556"]},"execution_count":96,"metadata":{},"output_type":"execute_result"}],"source":["textdistance.JaroWinkler().similarity('soccer', 'socrr')"]},{"cell_type":"code","execution_count":98,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["product with 0 similars: produto            saco pap 15kg 32x55   mil\n","ncm                                 48043190\n","produto_nfe        SACO PAP 15Kg 32x55 / MIL\n","w2v            [saco, pap, 15kg, 32x55, mil]\n","Name: 1358, dtype: object\n","\n"]}],"source":["db_final = pd.DataFrame(columns=columns_db_final)\n","for i in range(len(df_nfe)):\n","    \n","    score_final = 0\n","    score = 0\n","    score_w2v = 0\n","    score_similarity = 0\n","\n","    first_element = True\n","    idx = 0\n","\n","    ncm = df_nfe.loc[i, 'ncm']\n","    df_temp = df_base.loc[df_base['ncm'] == ncm]\n","\n","    if len(df_temp) > 0:\n","\n","        for j in range(len(df_temp)):\n","\n","            if first_element:\n","                \n","                score_w2v = average_sentence_similarity(\n","                    average_sentence2vec(df_nfe.loc[i, 'w2v']), \n","                    average_sentence2vec(df_temp.loc[df_temp.index[j], 'w2v']))\n","\n","                score_similarity = textdistance.JaroWinkler().similarity(df_nfe.loc[i, 'produto'],\n","                df_temp.loc[df_temp.index[j], 'produto'])\n","\n","                if score_similarity == 0:\n","                    score_similarity = 0.0001\n","\n","                score_final = 2 * ((score_w2v * score_similarity) / (score_w2v + score_similarity))\n","\n","                idx = df_temp.index[j]\n","                first_element = False\n","            else:\n","                score_w2v = average_sentence_similarity(\n","                    average_sentence2vec(df_nfe.loc[i, 'w2v']), \n","                    average_sentence2vec(df_temp.loc[df_temp.index[j], 'w2v']))\n","\n","                score_similarity = textdistance.JaroWinkler().similarity(df_nfe.loc[i, 'produto'],\n","                df_temp.loc[df_temp.index[j], 'produto'])\n","\n","                if score_similarity == 0:\n","                    score_similarity = 0.0001\n","\n","                score = 2 * ((score_w2v * score_similarity) / (score_w2v + score_similarity))\n","\n","            if score > score_final:\n","                score_final = score\n","                idx = df_temp.index[j]\n","\n","        # saving the best values in the dataframe\n","        try:\n","            db_final.loc[len(db_final.index)] = [\n","                df_temp.loc[idx, 'produto_base'],\n","                df_nfe.loc[i, 'produto_nfe'],\n","                df_temp.loc[idx, 'w2v'],\n","                df_nfe.loc[i, 'w2v'],\n","                df_nfe.loc[i, 'ncm'],\n","                score_final\n","            ]\n","        except:\n","            print(f\"something goes wrong: {df_temp.loc[idx,]}\\n{df_nfe.loc[i,]}\")\n","    \n","    else:\n","        print(f'product with 0 similars: {df_nfe.loc[i,]}\\n')\n","\n","db_final.to_csv('results/'+file_name+'.csv', encoding='utf-8')"]},{"cell_type":"markdown","metadata":{},"source":["**Most Similar - w2v weight + func**"]},{"cell_type":"code","execution_count":137,"metadata":{},"outputs":[{"data":{"text/plain":["'w_jarowink_w2v_v2w5s300e500'"]},"execution_count":137,"metadata":{},"output_type":"execute_result"}],"source":["file_name = \"w_jarowink_\"+model_name\n","file_name"]},{"cell_type":"code","execution_count":138,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["product with 0 similars: produto            saco pap 15kg 32x55   mil\n","ncm                                 48043190\n","produto_nfe        SACO PAP 15Kg 32x55 / MIL\n","w2v            [saco, pap, 15kg, 32x55, mil]\n","Name: 1358, dtype: object\n","\n"]}],"source":["db_final = pd.DataFrame(columns=columns_db_final)\n","for i in range(len(df_nfe)):\n","    \n","    score_final = 0\n","    score = 0\n","    score_w2v = 0\n","    score_similarity = 0\n","\n","    first_element = True\n","    idx = 0\n","\n","    ncm = df_nfe.loc[i, 'ncm']\n","    df_temp = df_base.loc[df_base['ncm'] == ncm]\n","\n","    if len(df_temp) > 0:\n","\n","        for j in range(len(df_temp)):\n","\n","            if first_element:\n","                \n","                score_w2v = weighted_sentence_similarity(\n","                    weighted_sentence2vec(df_nfe.loc[i, 'w2v']), \n","                    weighted_sentence2vec(df_temp.loc[df_temp.index[j], 'w2v']))\n","\n","                score_similarity = textdistance.JaroWinkler().similarity(df_nfe.loc[i, 'produto'],\n","                df_temp.loc[df_temp.index[j], 'produto'])\n","\n","                if score_similarity == 0:\n","                    score_similarity = 0.0001\n","\n","                score_final = 2 * ((score_w2v * score_similarity) / (score_w2v + score_similarity))\n","\n","                idx = df_temp.index[j]\n","                first_element = False\n","            else:\n","                score_w2v = weighted_sentence_similarity(\n","                    weighted_sentence2vec(df_nfe.loc[i, 'w2v']), \n","                    weighted_sentence2vec(df_temp.loc[df_temp.index[j], 'w2v']))\n","\n","                score_similarity = textdistance.JaroWinkler().similarity(df_nfe.loc[i, 'produto'],\n","                df_temp.loc[df_temp.index[j], 'produto'])\n","\n","                if score_similarity == 0:\n","                    score_similarity = 0.0001\n","\n","                score = 2 * ((score_w2v * score_similarity) / (score_w2v + score_similarity))\n","\n","            if score > score_final:\n","                score_final = score\n","                idx = df_temp.index[j]\n","\n","        # saving the best values in the dataframe\n","        try:\n","            db_final.loc[len(db_final.index)] = [\n","                df_temp.loc[idx, 'produto_base'],\n","                df_nfe.loc[i, 'produto_nfe'],\n","                df_temp.loc[idx, 'w2v'],\n","                df_nfe.loc[i, 'w2v'],\n","                df_nfe.loc[i, 'ncm'],\n","                score_final\n","            ]\n","        except:\n","            print(f\"something goes wrong: {df_temp.loc[idx,]}\\n{df_nfe.loc[i,]}\")\n","    \n","    else:\n","        print(f'product with 0 similars: {df_nfe.loc[i,]}\\n')\n","\n","db_final.to_csv('results/'+file_name+'.csv', encoding='utf-8')"]},{"cell_type":"markdown","metadata":{},"source":["**Gabarito - Importação**"]},{"cell_type":"code","execution_count":26,"metadata":{},"outputs":[],"source":["# gabarito da base de dados v1\n","cols_gab = ['id', 'produto_nfe', 'produto_cotriba']\n","# df_gabarito = pd.read_csv('C:/Users/higor/OneDrive/Área de Trabalho/word2vec/gabarito.csv', header = 0, \n","# names = cols_gab, engine = 'python', encoding = 'utf-8')\n","\n","# gabarito da base de dados com mais correções v2\n","df_gabarito = pd.read_csv('C:/Users/higor/OneDrive/Área de Trabalho/word2vec/gabarito_c2.csv', header = 0, \n","names = cols_gab, engine = 'python', encoding = 'utf-8')"]},{"cell_type":"code","execution_count":27,"metadata":{},"outputs":[],"source":["df_gabarito['produto_nfe'] = df_gabarito['produto_nfe'].astype(str)\n","# df_gabarito['produto_nfe'] = df_gabarito['produto_nfe'].apply(change_lower)\n","\n","df_gabarito['produto_cotriba'] = df_gabarito['produto_cotriba'].astype(str)\n","# df_gabarito['produto_cotriba'] = df_gabarito['produto_cotriba'].apply(change_lower)"]},{"cell_type":"markdown","metadata":{},"source":["**dataframe Average**"]},{"cell_type":"code","execution_count":112,"metadata":{},"outputs":[],"source":["# dataframe average v2\n","cols_dfw = ['produto_base', 'produto_nfe', 'w2v_base', 'w2v_nfe', 'ncm', 'score']\n","\n","df_average = pd.read_csv('C:/Users/higor/OneDrive/Área de Trabalho/word2vec/results/'+file_name+'.csv', header = 0, \n","names = cols_dfw, engine = 'python', encoding = 'utf-8')\n","\n","df_len = len(df_average)"]},{"cell_type":"code","execution_count":31,"metadata":{},"outputs":[],"source":["cols_dfw = ['produto_base', 'produto_nfe', 'w2v_base', 'w2v_nfe', 'ncm', 'score']\n","df_average = db_final\n","df_len = len(df_average)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df_average"]},{"cell_type":"markdown","metadata":{},"source":["**Average - F1**"]},{"cell_type":"code","execution_count":113,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["recuperados: 8369\n","encontrados: 9651\n","total: 9720\n"]}],"source":["count_total = 0\n","count_find = 0\n","for i in range(len(df_average)):\n","    nfe_equal = 0\n","    gabarito = False\n","    equivalente = df_average.loc[i]\n","    nfe_equal = df_gabarito.loc[df_gabarito['produto_nfe'] == equivalente['produto_nfe']]\n","\n","    if (len(nfe_equal) > 0):\n","        gabarito = df_gabarito.loc[nfe_equal.index,'produto_cotriba'].values == equivalente['produto_base']\n","\n","    if(len(nfe_equal) > 0 and (gabarito.all() == True)):\n","        count_find += 1\n","        count_total += 1\n","    elif (len(nfe_equal) > 0 and (gabarito.all() == False)):\n","        count_total += 1\n","\n","print(f'recuperados: {count_find}')\n","print(f'encontrados: {count_total}')\n","print(f'total: {df_len}')"]},{"cell_type":"code","execution_count":114,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["0.8671640244534246\n"]}],"source":["precisao = count_find / count_total\n","print(precisao)"]},{"cell_type":"code","execution_count":115,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["0.8610082304526749\n"]}],"source":["revocacao = count_find / df_len\n","print(revocacao)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["0.8640751639048061\n"]}],"source":["f1 = 2 * ( (precisao * revocacao) / (precisao + revocacao) )\n","print(f1)"]},{"cell_type":"markdown","metadata":{},"source":["**dataframe Weighted**"]},{"cell_type":"code","execution_count":139,"metadata":{},"outputs":[],"source":["cols_dfw = ['produto_base', 'produto_nfe', 'w2v_base', 'w2v_nfe', 'ncm', 'score']\n","\n","# dataframe weighted v1\n","# df_weighted = pd.read_csv('C:/Users/higor/OneDrive/Área de Trabalho/word2vec/results/word2vec_weighted.csv', header = 0, \n","# names = cols_dfw, engine = 'python', encoding = 'utf-8')\n","\n","# dataframe weighted v2\n","# df_weighted = pd.read_csv('C:/Users/higor/OneDrive/Área de Trabalho/word2vec/results/word2vec_weighted2.csv', header = 0, \n","# names = cols_dfw, engine = 'python', encoding = 'utf-8')\n","\n","# TESTE\n","df_weighted = pd.read_csv('C:/Users/higor/OneDrive/Área de Trabalho/word2vec/results/'+file_name+'.csv', header = 0, \n","names = cols_dfw, engine = 'python', encoding = 'utf-8')\n","\n","df_len = len(df_weighted)"]},{"cell_type":"code","execution_count":30,"metadata":{},"outputs":[],"source":["cols_dfw = ['produto_base', 'produto_nfe', 'w2v_base', 'w2v_nfe', 'ncm', 'score']\n","df_weighted = db_final\n","df_len = len(df_weighted)"]},{"cell_type":"markdown","metadata":{},"source":["**Weighted - F1**"]},{"cell_type":"code","execution_count":140,"metadata":{},"outputs":[],"source":["columns_weighted_gabarito = ['produto_base_w2v', 'produto_nfe_w2v',\n"," 'produto_base_gab', 'produto_nfe_gab', \n"," 'w2v_base', 'w2v_nfe', 'ncm', 'score']\n","df_weighted_gabarito = pd.DataFrame(columns=columns_weighted_gabarito)"]},{"cell_type":"code","execution_count":141,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["recuperados: 8329\n","encontrados: 9651\n","total: 9720\n"]}],"source":["count_total = 0\n","count_find = 0\n","for i in range(len(df_weighted)):\n","    nfe_equal = 0\n","    gabarito = False\n","    equivalente = df_weighted.loc[i]\n","    nfe_equal = df_gabarito.loc[df_gabarito['produto_nfe'] == equivalente['produto_nfe']]\n","\n","    if (len(nfe_equal) > 0):\n","        gabarito = df_gabarito.loc[nfe_equal.index,'produto_cotriba'].values == equivalente['produto_base']\n","\n","    if(len(nfe_equal) > 0 and (gabarito.all() == True)):\n","        count_find += 1\n","        count_total += 1\n","    elif (len(nfe_equal) > 0 and (gabarito.all() == False)):\n","        count_total += 1\n","        df_weighted_gabarito.loc[len(df_weighted_gabarito.index)] = [\n","                equivalente['produto_base'],\n","                equivalente['produto_nfe'],\n","                df_gabarito.loc[nfe_equal.index,'produto_cotriba'].values,\n","                df_gabarito.loc[nfe_equal.index,'produto_nfe'].values,\n","                equivalente['w2v_base'],\n","                equivalente['w2v_nfe'],\n","                equivalente['ncm'],\n","                equivalente['score'],\n","            ]\n","\n","print(f'recuperados: {count_find}')\n","print(f'encontrados: {count_total}')\n","print(f'total: {df_len}')\n","\n","    "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df_weighted_gabarito.to_csv('results/word2vec_weighted_gabarito4.csv', encoding='utf-8')"]},{"cell_type":"code","execution_count":142,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["0.8630193762304424\n"]}],"source":["precisao = count_find / count_total\n","print(precisao)"]},{"cell_type":"code","execution_count":143,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["0.8568930041152263\n"]}],"source":["revocacao = count_find / df_len\n","print(revocacao)"]},{"cell_type":"code","execution_count":144,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["0.8599452790253471\n"]}],"source":["f1 = 2 * ( (precisao * revocacao) / (precisao + revocacao) )\n","print(f1)"]},{"cell_type":"markdown","metadata":{},"source":["**dataframe Summed**"]},{"cell_type":"code","execution_count":51,"metadata":{},"outputs":[],"source":["# dataframe summed v2\n","cols_dfw = ['produto_base', 'produto_nfe', 'w2v_base', 'w2v_nfe', 'ncm', 'score']\n","df_summed = pd.read_csv('C:/Users/higor/OneDrive/Área de Trabalho/word2vec/results/'+file_name+'.csv', header = 0, \n","names = cols_dfw, engine = 'python', encoding = 'utf-8')\n","\n","df_len = len(df_summed)"]},{"cell_type":"code","execution_count":39,"metadata":{},"outputs":[],"source":["cols_dfw = ['produto_base', 'produto_nfe', 'w2v_base', 'w2v_nfe', 'ncm', 'score']\n","df_summed = db_final\n","df_len = len(df_summed)"]},{"cell_type":"markdown","metadata":{},"source":["**Summed - F1**"]},{"cell_type":"code","execution_count":52,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["recuperados: 8355\n","encontrados: 9651\n","total: 9720\n"]}],"source":["count_total = 0\n","count_find = 0\n","for i in range(len(df_summed)):\n","    nfe_equal = 0\n","    gabarito = False\n","    equivalente = df_summed.loc[i]\n","    nfe_equal = df_gabarito.loc[df_gabarito['produto_nfe'] == equivalente['produto_nfe']]\n","\n","    if (len(nfe_equal) > 0):\n","        gabarito = df_gabarito.loc[nfe_equal.index,'produto_cotriba'].values == equivalente['produto_base']\n","\n","    if(len(nfe_equal) > 0 and (gabarito.all() == True)):\n","        count_find += 1\n","        count_total += 1\n","    elif (len(nfe_equal) > 0 and (gabarito.all() == False)):\n","        count_total += 1\n","\n","print(f'recuperados: {count_find}')\n","print(f'encontrados: {count_total}')\n","print(f'total: {df_len}')"]},{"cell_type":"code","execution_count":53,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["0.8657133975753808\n"]}],"source":["precisao = count_find / count_total\n","print(precisao)"]},{"cell_type":"code","execution_count":54,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["0.8595679012345679\n"]}],"source":["revocacao = count_find / df_len\n","print(revocacao)"]},{"cell_type":"code","execution_count":55,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["0.8626297041969956\n"]}],"source":["f1 = 2 * ( (precisao * revocacao) / (precisao + revocacao) )\n","print(f1)"]},{"cell_type":"markdown","metadata":{},"source":["**dataframe Average + func**"]},{"cell_type":"code","execution_count":164,"metadata":{},"outputs":[{"data":{"text/plain":["'a_cosine_w2v_v2w7s300e750'"]},"execution_count":164,"metadata":{},"output_type":"execute_result"}],"source":["teste_name = 'a_cosine_'+model_name\n","teste_name"]},{"cell_type":"code","execution_count":165,"metadata":{},"outputs":[],"source":["# dataframe average + func\n","cols_dfw = ['produto_base', 'produto_nfe', 'w2v_base', 'w2v_nfe', 'ncm', 'score']\n","\n","df_average_func = pd.read_csv('C:/Users/higor/OneDrive/Área de Trabalho/word2vec/results/'+teste_name+'.csv', header = 0, \n","names = cols_dfw, engine = 'python', encoding = 'utf-8')\n","\n","df_len = len(df_average_func)"]},{"cell_type":"code","execution_count":139,"metadata":{},"outputs":[{"data":{"text/plain":["'w_jarowink_w2v_v2w5s300e500'"]},"execution_count":139,"metadata":{},"output_type":"execute_result"}],"source":["file_name"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df_average_func"]},{"cell_type":"code","execution_count":166,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["recuperados: 8487\n","encontrados: 9651\n","total: 9720\n"]}],"source":["count_total = 0\n","count_find = 0\n","for i in range(len(df_average_func)):\n","    nfe_equal = 0\n","    gabarito = False\n","    equivalente = df_average_func.loc[i]\n","    nfe_equal = df_gabarito.loc[df_gabarito['produto_nfe'] == equivalente['produto_nfe']]\n","\n","    if (len(nfe_equal) > 0):\n","        gabarito = df_gabarito.loc[nfe_equal.index,'produto_cotriba'].values == equivalente['produto_base']\n","\n","    if(len(nfe_equal) > 0 and (gabarito.all() == True)):\n","        count_find += 1\n","        count_total += 1\n","    elif (len(nfe_equal) > 0 and (gabarito.all() == False)):\n","        count_total += 1\n","\n","print(f'recuperados: {count_find}')\n","print(f'encontrados: {count_total}')\n","print(f'total: {df_len}')"]},{"cell_type":"code","execution_count":167,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["0.8793907367112216\n"]}],"source":["precisao = count_find / count_total\n","print(precisao)"]},{"cell_type":"code","execution_count":168,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["0.8731481481481481\n"]}],"source":["revocacao = count_find / df_len\n","print(revocacao)"]},{"cell_type":"code","execution_count":169,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["0.8762583242992101\n"]}],"source":["f1 = 2 * ( (precisao * revocacao) / (precisao + revocacao) )\n","print(f1)"]},{"cell_type":"markdown","metadata":{},"source":["**Cosine - textual**"]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[],"source":["cols = ['produto', 'ncm']\n","# cols2 = ['produto' 'ncm']\n","df_base = pd.read_csv('C:/Users/higor/OneDrive/Área de Trabalho/db_tcc/cotri-nome-ncm.csv', header = 0, \n","names = cols, engine = 'python', encoding = 'utf-8')\n","df_nfe = pd.read_csv('C:/Users/higor/OneDrive/Área de Trabalho/db_tcc/nfe-nome-ncm.csv', header = 0, \n","names = cols, engine = 'python', encoding = 'utf-8')"]},{"cell_type":"markdown","metadata":{},"source":["**Pré-processamento**"]},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[],"source":["def clean_data_text(text):\n","    try:\n","        text = unicode(text, 'utf-8')\n","    except NameError:\n","        pass\n","\n","    text = unicodedata.normalize('NFD', text)\\\n","           .encode('ascii', 'ignore')\\\n","           .decode(\"utf-8\")\n","\n","    text = text.translate(str.maketrans(string.punctuation, ' '*len(string.punctuation)))\n","    text = text.lower()\n","    text = re.sub('\\s\\s+', ' ', text.strip())\n","    return str(text)"]},{"cell_type":"code","execution_count":23,"metadata":{},"outputs":[],"source":["# pre processing database\n","df_base['ncm'] = df_base['ncm'].replace('UPDT1', 0)\n","df_base['ncm'] = df_base['ncm'].fillna(0)\n","df_base['ncm'] = df_base['ncm'].astype(int)\n","df_base['produto_base'] = df_base['produto']\n","df_base['produto'] = df_base['produto'].apply(clean_data_text)\n","\n","# pre processing nfe\n","df_nfe['ncm'] = df_nfe['ncm'].astype(int)\n","df_nfe['produto_nfe'] = df_nfe['produto']\n","df_nfe['produto'] = df_nfe['produto'].apply(clean_data_text)"]},{"cell_type":"code","execution_count":97,"metadata":{},"outputs":[],"source":["columns_cos = ['produto_base', 'produto_nfe', 'produto_base_pp', 'produto_nfe_pp', 'ncm', 'score']\n","db_final = pd.DataFrame(columns=columns_cos)"]},{"cell_type":"code","execution_count":98,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>produto_base</th>\n","      <th>produto_nfe</th>\n","      <th>produto_base_pp</th>\n","      <th>produto_nfe_pp</th>\n","      <th>ncm</th>\n","      <th>score</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["Empty DataFrame\n","Columns: [produto_base, produto_nfe, produto_base_pp, produto_nfe_pp, ncm, score]\n","Index: []"]},"execution_count":98,"metadata":{},"output_type":"execute_result"}],"source":["db_final"]},{"cell_type":"code","execution_count":99,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["product with 0 similars: produto          saco pap 15kg 32x55 mil\n","ncm                             48043190\n","produto_nfe    SACO PAP 15Kg 32x55 / MIL\n","Name: 1358, dtype: object\n","\n"]}],"source":["for i in range(len(df_nfe)):\n","    \n","    score_final = 0\n","    score = 0\n","    first_element = True\n","    idx = 0\n","\n","    ncm = df_nfe.loc[i, 'ncm']\n","    df_temp = df_base.loc[df_base['ncm'] == ncm]\n","    \n","    if len(df_temp) > 0:\n","\n","        for j in range(len(df_temp)):\n","\n","            if first_element:\n","                score_final = textdistance.Overlap().similarity(df_nfe.loc[i, 'produto'],\n","                df_temp.loc[df_temp.index[j], 'produto'])\n","                idx = df_temp.index[j]\n","                first_element = False\n","            else:\n","                score = textdistance.Overlap().similarity(df_nfe.loc[i, 'produto'],\n","                df_temp.loc[df_temp.index[j], 'produto'])\n","                \n","            if score > score_final:\n","                score_final = score\n","                idx = df_temp.index[j]\n","\n","        # saving the best values in the dataframe\n","        try:\n","            # db_final.loc[len(db_final.index)] = [\n","            #     df_temp.loc[idx, 'produto'],\n","            #     df_nfe.loc[i, 'produto'],\n","            #     df_nfe.loc[i, 'ncm'],\n","            #     score_final\n","            # ]\n","\n","            db_final.loc[len(db_final.index)] = [\n","                df_temp.loc[idx, 'produto_base'],\n","                df_nfe.loc[i, 'produto_nfe'],\n","                df_temp.loc[idx, 'produto'],\n","                df_nfe.loc[i, 'produto'],\n","                df_nfe.loc[i, 'ncm'],\n","                score_final\n","            ]\n","        except Exception as e:\n","            print(str(e))\n","            print(f\"something goes wrong: {df_temp.loc[idx,'produto_base']}\\n{df_nfe.loc[i,'produto_nfe']}\")\n","    \n","    else:\n","        print(f'product with 0 similars: {df_nfe.loc[i,]}\\n')\n","\n","db_final.to_csv('results/overlap4.csv', encoding='utf-8')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["db_final"]},{"cell_type":"code","execution_count":58,"metadata":{},"outputs":[],"source":["# gabarito da base de dados com mais correções v2\n","cols_gab = ['id', 'produto_nfe', 'produto_cotriba']\n","df_gabarito = pd.read_csv('C:/Users/higor/OneDrive/Área de Trabalho/word2vec/gabarito_c2.csv', header = 0, \n","names = cols_gab, engine = 'python', encoding = 'utf-8')"]},{"cell_type":"code","execution_count":59,"metadata":{},"outputs":[],"source":["df_gabarito['produto_nfe'] = df_gabarito['produto_nfe'].astype(str)\n","# df_gabarito['produto_nfe'] = df_gabarito['produto_nfe'].apply(change_lower)\n","\n","df_gabarito['produto_cotriba'] = df_gabarito['produto_cotriba'].astype(str)\n","# df_gabarito['produto_cotriba'] = df_gabarito['produto_cotriba'].apply(change_lower)"]},{"cell_type":"code","execution_count":100,"metadata":{},"outputs":[],"source":["# TESTE\n","co_cosine = ['produto_base', 'produto_nfe', 'ncm', 'score']\n","co_cosine2 = ['produto_base', 'produto_nfe','produto_base_pp', 'produto_nfe_pp', 'ncm', 'score']\n","\n","df_cos = pd.read_csv('C:/Users/higor/OneDrive/Área de Trabalho/word2vec/results/overlap4.csv', header = 0, \n","names = co_cosine2, engine = 'python', encoding = 'utf-8')\n","\n","df_len = len(df_cos)"]},{"cell_type":"code","execution_count":101,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["recuperados: 6775\n","encontrados: 9651\n","total: 9720\n"]}],"source":["count_total = 0\n","count_find = 0\n","for i in range(len(df_cos)):\n","    nfe_equal = 0\n","    gabarito = False\n","    equivalente = df_cos.loc[i]\n","    nfe_equal = df_gabarito.loc[df_gabarito['produto_nfe'] == equivalente['produto_nfe']]\n","\n","    if (len(nfe_equal) > 0):\n","        gabarito = df_gabarito.loc[nfe_equal.index,'produto_cotriba'] == equivalente['produto_base']\n","    if(len(nfe_equal) > 0 and (gabarito.all() == True)):\n","        count_find += 1\n","        count_total += 1\n","    elif (len(nfe_equal) > 0 and (gabarito.all() == False)):\n","        count_total += 1\n","\n","print(f'recuperados: {count_find}')\n","print(f'encontrados: {count_total}')\n","print(f'total: {df_len}')\n"]},{"cell_type":"code","execution_count":102,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["0.7019997927675888\n"]}],"source":["precisao = count_find / count_total\n","print(precisao)"]},{"cell_type":"code","execution_count":103,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["0.6970164609053497\n"]}],"source":["revocacao = count_find / df_len\n","print(revocacao)"]},{"cell_type":"code","execution_count":104,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["0.6994992514583656\n"]}],"source":["f1 = 2 * ( (precisao * revocacao) / (precisao + revocacao) )\n","print(f1)"]},{"cell_type":"markdown","metadata":{},"source":["**Testes**"]},{"cell_type":"code","execution_count":36,"metadata":{},"outputs":[{"data":{"text/plain":["(300,)"]},"execution_count":36,"metadata":{},"output_type":"execute_result"}],"source":["w2v_model.wv[\"sabao\"].shape"]},{"cell_type":"code","execution_count":31,"metadata":{},"outputs":[{"data":{"text/plain":["[('puravi', 0.5243518352508545),\n"," ('grostolli', 0.5223467946052551),\n"," ('congelad', 0.5142188668251038),\n"," ('tickroc', 0.4930357038974762),\n"," ('sovital', 0.4914597272872925),\n"," ('grostoli', 0.49016380310058594),\n"," ('sovado', 0.4899028241634369),\n"," ('baguete', 0.48934367299079895),\n"," ('cabritinho', 0.48825597763061523),\n"," ('cervejinha', 0.48750758171081543)]"]},"execution_count":31,"metadata":{},"output_type":"execute_result"}],"source":["w2v_model.wv.most_similar(positive=[\"pao\"])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(sum_sentence2vec(w2v_df[0]))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(average_sentence2vec(w2v_df[0]))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(weighted_sentence2vec(w2v_df[0]))"]},{"cell_type":"code","execution_count":27,"metadata":{},"outputs":[{"data":{"text/plain":["0.6614604"]},"execution_count":27,"metadata":{},"output_type":"execute_result"}],"source":["average_sentence_similarity(average_sentence2vec(w2v_df[510]), average_sentence2vec(w2v_df[1377]))\n"]},{"cell_type":"code","execution_count":28,"metadata":{},"outputs":[{"data":{"text/plain":["0.6614604"]},"execution_count":28,"metadata":{},"output_type":"execute_result"}],"source":["sum_sentence_similarity(sum_sentence2vec(w2v_df[510]), sum_sentence2vec(w2v_df[1377]))\n"]},{"cell_type":"code","execution_count":32,"metadata":{},"outputs":[{"data":{"text/plain":["0.6152559"]},"execution_count":32,"metadata":{},"output_type":"execute_result"}],"source":["weighted_sentence_similarity(weighted_sentence2vec(w2v_df[510]), weighted_sentence2vec(w2v_df[1377]))"]}],"metadata":{"interpreter":{"hash":"b0a4987a4591b867e1aec3871a557fb021a248a657357c8ea438a34cb6c143c5"},"kernelspec":{"display_name":"Python 3.9.12 ('nlp')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.12"}},"nbformat":4,"nbformat_minor":4}
